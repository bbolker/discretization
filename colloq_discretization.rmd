---
title: Nothing is zero in a multifactorial world \newline (and why it matters)
author: "Ben Bolker \\newline McMaster University"
date: "17 September 2021"
bibliography: discrete.bib
csl: reflist2.csl
output:
 beamer_presentation:
  includes:
   in_header: ./header.tex
   after_body: ./suffix.tex
  slide_level: 2
  keep_tex: true
---
<!-- need to rmarkdown::render(), make rule doesn't get the header properly! -->
<!-- 
tex hacks required: remove empty frame at beginning; break line 
  in title (Burnham ref; add \framebreak manually to refs (ugh) -->
<!-- 
apa.csl is a slightly hacked version of APA 
  (modified for "et al" after 2 authors in text)
  -->
<!-- blockquote:
  https://css-tricks.com/snippets/css/simple-and-nice-blockquote-styling/ -->
<!-- center:
    https://www.w3schools.com/howto/howto_css_image_center.asp -->
<!-- .refs is style for reference page (small text) -->
<style>
.refs {
font-size: 14px;
}
.sm_block {
 font-size: 20px;
}
h2 { 
 color: #3399ff;		
}
h3 { 
 color: #3399ff;		
}
.title-slide {
   background-color: #55bbff;
   }
blockquote {
  background: #f9f9f9;
  border-left: 10px solid #ccc;
  margin: 1.5em 10px;
  padding: 0.5em 10px;
  quotes: "\201C""\201D"
}
blockquote:before {
  color: #ccc;
  content: open-quote;
  font-size: 4em;
  line-height: 0.1em;
  margin-right: 0.25em;
  vertical-align: -0.4em;
}
blockquote p {
  display: inline;
}
.center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 50%;
}
</style>
<!--    content: url(https://i.creativecommons.org/l/by-sa/4.0/88x31.png)
>
<!-- Limit image width and height -->
<style type="text/css">
img {     
  max-height: 560px;     
  max-width: 700px; 
}
div#before-column p.forceBreak {
	break-before: column;
}

div#after-column p.forceBreak {
	break-after: column;
}
</style>
```{r setup,echo=FALSE,message=FALSE, include=FALSE}
library(tidyverse)
library("ggplot2"); theme_set(theme_classic())
library("reshape2")
library("ggExtra")
library("MASS")
library("knitr")
opts_chunk$set(echo=FALSE,fig.width=4,fig.height=4, out.height="0.8 \\textheight")
## https://www.mattblackwell.org/blog/2021/04/06/rmd-overlays/
hook_plot <- knitr::knit_hooks$get("plot")
knitr::knit_hooks$set(
  plot = function(x, options) {
    if (is.null(options$overlay.plot)) {
      return(hook_plot(x, options))
    } else {
      i <- options$fig.cur
      hand <- as.numeric(ifelse(i == options$overlay.plot, 1, 0))
      bf <- paste0("\\only<", i, "| handout:", hand, ">{")
      paste(c(bf, knitr::hook_plot_tex(x, options), "}"), collapse = "\n")
    }
  }
)
```
## acknowledgements

money: NSERC

ideas: Jonathan Dushoff, Marm Kilpatrick, Brian McGill, Daniel Park, Daniel Turek

## what is a multifactorial system?

- many processes contribute to pattern
- quantify *how* each process affects the system,  
rather than testing *whether* we can detect its impact

This talk is mostly about *inferential* or *scientific* questions

## what are we trying to do?

- **prediction**: what do we expect in a specified scenario?
- **inference**: what factors are affecting observed outcomes, and how much?
   - (or: prediction/estimation of effects \cemph{with appropriate uncertainty bounds})
   
## why do we pretend some effects are zero?

 we often consider \cemph{point hypotheses}, often as *counterfactual null hypotheses*
 
- e.g.: "the delta variant has the same $R_0$ as earlier variants"; "masks do not prevent COVID transmission"
- never\footnotemark true in biology/psychology/economics/epidemiology ...

more generally we might want to test among \emph{discrete} hypotheses:

- "strong inference" [@platt_strong_1964]:  
(binary, discrete) alternative hypotheses 
- testing among *many* discrete hypotheses  
[@taper_evidential_2015]

\footnote[1]{what, never?}

## the method of multiple working hypotheses

Chamberlin's \emph{method of multiple working hypotheses}  
[@raup_method_1995]

<blockquote>
... the measure of participation of each [process] must be determined before a satisfactory elucidation can be reached. The full solution therefore involves not only a recognition of multiple participation but an estimate of the measure and mode of each participation ...
</blockquote>

## discretomania

- null hypothesis testing as measure of \emph{clarity}  
(\bemph{OK} @dushoff_i_2019)
- "feature/variable selection" in data science  
(\bemph{OK but maybe not optimal?})
- stepwise regression (\cemph{not OK!})

What is the goal of stepwise regression, and why is it a problem?

<!-- animation?

https://gist.github.com/mattblackwell/0d26d5c8f61f231570d61ccd62fe511f 
https://www.mattblackwell.org/blog/2021/04/06/rmd-overlays/

-->

## overfitting (1)

```{r overfit_calc}
## http://www.mathworks.com/products/matlab/demos.html?
##  file=/products/demos/shipping/matlab/census.html
dd <- data.frame(time = seq(1900,2000,by=10),
           pop = c(75.995,91.972,105.711,123.203,131.669,
                   150.697,179.323,203.212,226.505,249.633,281.422)) %>%
    mutate(sct=(time-1950)/50)
predframe <- tibble(time=seq(1900,2020, length=101)) %>% mutate(sct=(time-1950)/50)
orders = c(1:4,8)
pred <- purrr::map_dfr(setNames(orders, orders),
               ~ {
                   m <- lm(pop ~ poly(sct, ., raw=TRUE), data=dd)
                   data.frame(predframe,
                              pop = predict(m, newdata=predframe))
               },
               .id = "order")
```

```{r overfit_plot, overlay.plot=5, fig.width=5}
gg_full <- ggplot(pred, aes(time, pop)) +
    geom_point(data = dd, size=3) +
    geom_line(aes(colour=order)) +
    coord_cartesian(ylim=c(0,350),
                    xlim=c(1890, 2025), expand=FALSE) +
    labs(x="year", y="US population (millions)")
for (i in seq_along(orders)) {
    print(gg_full %+% filter(pred, order %in% orders[1:i]))
}
```

## overfitting/underfitting {.columns-2 .build}

\begincols
\begincol{0.5\textwidth}

- \onslide<1-> avoid omitting important predictors
- \onslide<2-> avoid overfitting
- \onslide<3-> $\approx$ optimize \cemph{bias-variance tradeoff}

\endcol
\begincol{0.5\textwidth}

\onslide<1->
<!-- \pause -->

\includegraphics[width=\textwidth]{pix/330px-The_Three_Bears_-_Project_Gutenberg_eText_17034.jpg}

\tiny Rackham 1837

\endcol
\endcols

## no free lunches

- data-based approaches to variable selection risk the \cemph{Texas sharpshooter fallacy}
- we are choosing to set a subset of model coefficients/effects to zero
- can we do better?
- \cemph{model averaging}: instead of picking one model, combine them all (*shrinkage estimator*)

## multimodel averaging  (MMA) [@burnham_model_2002]

- ``information-theoretic" approach (Kullback-Leibler distance): 
- typical (ecologist's) approach
    - fit full model
	- fit all (or many) submodels
	- compute weights
    - compute model-averaged point estimates and CIs

## why *not* use MMA?

three problems:

- conceptual
- computational
- inferential

## conceptual problem: discretization {.build .columns-2}

\begincols

\begincol{0.5\textwidth}

- information-theoretic (IT) approaches framed as implementation of Chamberlin [@elliott_revisiting_2007]
- but IT focuses on differentiating **discrete** hypotheses/models
- submodels are straw men
- Â¿ is this actually a problem ?

\endcol
\begincol{0.5\textwidth}

\pause
\includegraphics[width=\textwidth]{pix/eyam_ternary.png}

{\small Estimated contribution of plague transmission modes in Eyam 1665: SW Park}

\endcol
\endcols

## computational problem: efficiency {.columns-2 .build}

\begincols
\begincol{0.5\textwidth}

\newcommand{\bigO}{{\mathcal{O}}}

- model average may mean fitting **lots** of models: for $K$ variables with $n$ observations, $\bigO(2^K K^2 n)$
- we do have lots of computers ...

\endcol
\begincol{0.5\textwidth}

\pause

\includegraphics[width=0.75\textwidth]{pix/bitcoin_nyc.png}

\endcol
\endcols

## inferential problem: undercoverage {.build}

- various methods for constructing MMA CIs [@burnham_model_2002;@fletcher2012model;@kabaila_model-averaged_2016]
- MMA CIs are generally **too narrow**  
[@turek2013frequentist;@kabaila_model-averaged_2016;@dormann_model_2018] but cf. @burnham_model_2002
- general property of shrinkage estimators? e.g. ridge-regression CI width $\geq$ least-squares CI [@obenchain_classical_1977]


## undercoverage (2)

- **no free lunch**; can we ever gain certainty by (data-driven) shrinkage?
- **coverage**: proportion of time that the confidence interval includes the true value (should be 95% for 95% CIs)

\includegraphics[width=0.9\textwidth]{pix/dormann_coverage.png}

@dormann_model_2018, Figure 5

## better shrinkage estimators

- most machine learning approaches use shrinkage somehow
- lasso & ridge regression; random forests; Bayesian priors
- complexity may be $\bigO(K^2 n)$ or $\bigO(K n \log(n))$ (vs $\bigO(2^K K^2 n)$ for brute force)
[@hardy_machine_2017; @louppe_understanding_2014]  
- but inference (i.e., confidence intervals) is still hard!
   - **high-dimensional inference**: narrow scope, strong assumptions
	
## conclusions: what should you do?

- for **inference**:
    - use the full model
    - *a priori* model reduction [@harrell_regression_2001]
- for **prediction**:
    - use appropriate shrinkage estimators, but beware CIs

## References {.refs .columns-2 .allowframebreaks}

\tiny



