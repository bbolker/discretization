---
title: "Multimodel approaches are not the best way to understand multifactorial systems"
author: Ben Bolker
output: pdf_document
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
header-includes:
   - \usepackage{palatino}
geometry: margin=1.5in
fontsize: 12pt
bibliography: discrete.bib
---

### Abstract

Information-theoretic (IT) and multi-model averaging (MMA) statistical approaches are suboptimal tools for pursuing a multifactorial approach (the "method of multiple working hypotheses") in ecology. (1) Conceptually, IT encourages ecologists to perform tests on sets of straw-man models. (2) MMA improves on IT model comparison; it represents a simple form of *shrinkage estimation* (a way to make accurate predictions from a model with many parameters, by "shrinking" parameter estimates toward zero). However, newer approaches to shrinkage estimation are faster and have more rigorous statistical underpinnings. (3) A major disadvantage of shrinkage methods is that confidence intervals are extremely difficult to derive. If researchers want accurate estimates of the strength of multiple competing ecological processes along with reliable confidence intervals, their best hope is to use full (maximal) statistical models after making principled, *a priori* decisions about which predictors to include.

----

Many modern ecological and evolutionary studies try to quantify the strength and importance of multiple processes in ecological systems: for example, the effects of herbivory and fertilization on standing biomass [@Gruner+2008]; the effects of bark, wood density, and fire on tree mortality [@brando_fire-induced_2012]; or the effects of taxonomic and genomic position on evolutionary rates [@ghenu_multicopy_2016]. This *multifactorial* approach [@mcgill_why_2016] complements, rather than replacing, the traditional hypothesis-testing or strong-inferential framework [@platt_strong_1964;@fox_why_2016].^[While there is much interesting debate over the best methods for gathering evidence to distinguish among two or more particular, *intrinsically* discrete hypotheses [@taper_evidential_2015], that is not my focus here.]

A standard statistical approach to analyzing multifactorial systems, particularly common in wildlife and conservation ecology, goes as follows: (1) Construct a full model that encompasses as many of the processes (and their interactions) as is feasible. (2) Fit the full model and check that it describes the data reasonably well (e.g. by computing $R^2$ values or estimating overdispersion). (3) Construct all, or most, of the possible submodels of the full model by setting subsets of parameters to zero. (4) If one model clearly dominates the ensemble of models, draw conclusions based on it. Otherwise, either (a) use multi-model averaging to estimate model-averaged parameters and confidence intervals or (b) draw conclusions about the importance of different processes either informally, by seeing which parameters are contained in the best (lowest-AIC) models, or formally, by summing the AIC weights. [CITATION?] I claim that this approach, even if used sensibly (e.g. with reasonably small numbers of predictors and submodels)

Our goal is to tease apart the contributions of many processes, *all* of which we believe are affecting the populations and communities we study to some degree. If our scientific question is (something like) "How important is this factor, in an absolute sense or relative to other factors?", not "Which of these factors are actually doing *anything at all* in my system?", why are we working so hard to fit many models of which only one (the full model) incorporates all of the factors? If we are not really interested (at the moment) in particular discrete hypotheses about our system, why does so much of our data-analytic effort go into various ways to test between, or combine and reconcile, multiple discrete models? This is an "XY problem"^[http://www.perlmonks.org/?node=XY+Problem]: rather than thinking about the best way to solve our real problem $X$ (understanding multifactorial systems), we have gotten bogged down in the details of how to make a particular tool, $Y$ (multimodel approaches) provide the answers we need.

One legitimate reason to fit multiple models is as one step in a null-hypothesis significance testing (NHST) procedure. While much maligned, NHSTs are a useful part of data analysis --- *not* to decide whether we really think a null hypothesis is false (they almost always are), but to see if we can reliably determine the *direction of effects* of ecological processes --- that is, not whether a parameter is zero, but whether we can tell unequivocally that it is negative or positive^[Thanks to J. Dushoff for this perspective on NHST.]. We can perform these tests by statistically comparing a full model to a reduced model that pretends the effect is exactly zero.

However, ecologists pursuing multimodel approaches are not just fitting one-step-reduced models to test hypotheses; they are fitting *all* of the reduced models. Their usual motivation is a hope that multimodel averaging will help them deal with insufficient data in a multifactorial world. If we had enough information (even "big data" doesn't always provide as much information as we need), we could fit just the full model, drawing our conclusions from the estimates and confidence intervals (CIs) with all of the factors considered simultaneously. But we always have too many predictors, and not enough data; we don't want to overfit (which will inflate our CIs and p-values to the point where we can't tell anything for sure), but at the same time we are scared of neglecting potentially important effects.

Stepwise regression, the original strategy for separating signals from noise, is now widely deprecated [@harrell_regression_2001;@whittingham_why_2006]^[Although it may sometimes be adequate for selecting a single best model for prediction [@murtaugh_performance_2009].]. Since the mid-1990s, ecologists have adopted information-theoretic approaches [@BurnAnde98]. These tools mitigate the instability of stepwise approaches, allow simultaneous comparison of many, non-nested models, and avoid the stigma of NHST. A further step forward, multi-model averaging [@burnham_model_2002] accounts for model uncertainty and avoids focusing on a single best model. More recently, however, model averaging is experiencing a backlash, as statistically savvy ecologists point out that multimodel averaging runs into trouble whenever variables are collinear [@freckleton_dealing_2011]; when we are careless about the meaning of main effects in the presence of interactions; when we average model parameters on the way to predicting nonlinear functions of the parameters [@cade_model_2015]; and when we use summed model weights to assess the relative importance of predictors [@galipaud_ecologists_2014].

Rather than using information criteria as tools to identify the best predictive model, or to obtain the best overall (model-averaged) predictions, most users of information criteria are using them either to (dubiously) quantify variable importance [@galipaud_ecologists_2014], or, by multimodel averaging, to have their cake and eat it too --- to avoid either over- or underfitting while quantifying effects in multifactorial systems.
They encounter two problems, one conceptual and one practical.

Conceptually, many of the difficulties of model averaging come from the original sin of unnecessarily dichotomizing a continuous world. Suppose we want to understand the effects of temperature and precipitation on biodiversity. The model-comparison or model-averaging approach would construct five models: a null model with no effects of either temperature or precipitation, two single-factor models, an additive model, and a full model allowing for interactions between temperature and precipitation. We would then fit all of these models and then model-average their parameters. We might be doing this in an effort to get good predictions, or to to test our confidence that we know the signs of particular effects (measured in the context of whatever processes are included in the reduced and the full models), but they are only means to an end, and we certainly shouldn't fool ourselves into thinking that we are using the method of multiple working hypotheses. For example, Chamberlin argued that in teaching about the origin of the Great Lakes we should urge students "to conceive of three or more great agencies [pre-glacial erosion, glacial erosion, crust deformation] working successively or simultaneously, and to estimate how much was accomplished by each of these agencies." Chamberlin was *not* suggesting that we test which individual mechanism or combination of mechanism fits the data best (in whatever sense), but instead that we acknowledge that the world is multifactorial.

Practically, model averaging is slow. Individual models can take minutes or hours to fit, and we may have to fit dozens or scores of sub-models in the multi-model averaging process. There are efficient tools available for fitting "right-sized" models that avoid many of the technical problems of model averaging. Penalized methods such as ridge and lasso regression [@dahlgren_alternative_2010] are well known outside of ecology; in a Bayesian setting, informative priors centered at zero have the same effect of *regularizing* --- pushing weak effects toward zero and controlling model complexity. Developed specifically for optimal (predictive) fitting in models with many parameters, these models have well-understood statistical properties; they avoid the pitfalls of model-averaging correlated or nonlinear parameters; and, by avoiding the need to fit many sub-models in the model-averaging processes, they are much faster.^[Although they often require a computationally expensive cross-validation step in order to choose the degree of penalization.]

Here I am not tackling the issue of whether 'truth' is included in our model set (it isn't), and how this matters to our inference [@barker_truth_2015]. I am claiming the opposite, that our full model is usually about as close to truth as we can get; we don't really believe any of the less complex models. If we are trying to get the best predictions, or to compare the strength of various processes in a multifactorial context, there may be better ways to do it. In situations where we really want to compare qualitatively different, non-nested hypotheses [@luttbeg_comparing_2004], AIC or BIC or any appropriate model-comparison tool is fine; however, if the models are *really* qualitatively different, perhaps we shouldn't be trying to merge them by averaging.

Penalized models have their own problems. Although powerful computational tools exist for fitting penalized versions of linear and generalized linear models (e.g. the `glmnet` package for R) and mixed models (`glmmLasso`), software for some of the more exotic models used by ecologists (e.g. zero-inflated models) may not be readily available. Fitting these models requires the user to choose the degree of penalization; although this process is conveniently automated in tools like `glmnet`, it may be
tricky for data that are correlated in space or time [@wenger_assessing_2012]. Finally, computing
CIs for parameters in penalized models --- one of the most basic outputs we need from a statistical analysis of
a multifactorial system --- is a current research problem; statisticians have proposed methods for deriving CIs  [@potscher_confidence_2008;@lee_exact_2013;@javanmard_confidence_2014;@lockhart_significance_2014], but they are far from being standard options in software. Ecologists should encourage their quantitatively savvy friends to build tools that make penalized approaches easier to use.

Statisticians derived confidence intervals for ridge regression long ago [@obenchain_classical_1977] --- but, paradoxically, they are identical to the confidence intervals one would have gotten from the full model without penalization! More recently, work by Turek improving and evaluating different methods for constructing multi-model averaged confidence intervals [@turek2012model;@fletcher2012model;@turek2013frequentist;@turek2015comparison] shows that multi-model averaged confidence intervals are nearly always too narrow; in simulations, 95% confidence intervals constructed according to even the best multi-model averaging algorithm may typically include the true parameter values only about 80% of the time. Free lunches do not exist in statistics, any more than anywhere else. We can use penalized approaches to improve prediction accuracy without having to sacrifice any input variables (by trading bias for variance), but the *only* way to gain statistical power for testing hypotheses, or narrowing or uncertainty about our predictions, is to limit the scope of our models *a priori* [@harrell_regression_2001] --- or to collect more data.

If we have good experimental designs and sensible scientific questions, muddling through with existing techniques will give us reasonable results, most of the time [@murtaugh_performance_2009]. But ecologists should be aware that the roundabout statistical methods they currently rely on to understand multifactorial
systems are *not* designed for those purposes; they were developed for prediction rather than inference.
When prediction is the primary goal, penalized methods can work better (faster and with better-known properties) than multimodel averaging. When estimating the magnitude of effects or judging variable importance, penalized methods may be appropriate --- or we may have to go back to the difficult choice of focusing on a restricted number of variables for which we have enough to data to fit and interpret the full model.

## Acknowledgments

Dana Karelus, Daniel Turek, Jeff Walker, ...

## Additional refs

- [@fieberg_mmi_2015;@walker_defense_2017;@nilsen_confidence_2005;@lukacs_model_2010]
- shrinkage estimators: van Houwelingen 2001 *Statistica Neerlandica*, Hooten and Hobbs 2015 *Eco Monographs* (Bayesian model selection)
- multi-model averaging:  Claeskens and Carroll 2007 *Biometrika*, Wang and Zhou 2013 *Communications in Statistics*; @claeskens_model_2008
- note critique of Galipaud et al.

Interestingly, the model averaged variance is smaller than the stepwise selected variance, yet produces a confidence interval with better coverage because of the reduced bias. Therefore, model averaging was better here than stepwise selection in terms of both bias and coverage in the case of weakly correlated variables.

From @kabaila_model-averaged_2016:
\newcommand{\mm}{\ensuremath $\cal M$_1}
> An ideal confidence interval should have minimal coverage equal to its nominal coverage and, to show a benefit of model selection, have scaled expected length that (a) is substantially less than 1 under $\mm$, (b) has a maximum value that [is] not much larger than 1 and (c) is close to 1 if the data happens to strongly contradict the model $\mm$. The results of the present paper and the fact that the Hjort & Claeskensâ€™ (2003) interval turns out to be essentially the same as the standard confidence interval based on fitting a full model suggest that this will be very difficult to achieve using model averaged confidence intervals. In other words, it seems difficult to find model-averaged confidence intervals that compete successfully with the standard confidence interval based on the full model.

# References

